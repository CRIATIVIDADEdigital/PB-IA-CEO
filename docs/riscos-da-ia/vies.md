---
sidebar_position: 2
---
# Viés
Uma expressão popular entre os primeiros desenvolvedores de sistemas décadas atrás dizia: 'lixo entra, lixo sai'. É uma referência ao conceito de que dados de entrada defeituosos ou sem sentido em sistemas computacionais resultam em saídas igualmente deficientes, independentemente da qualidade do sistema em si. Esse mesmo princípio pode ser aplicado ao treinamento de IA: a qualidade do modelo é tão boa quanto os dados usados para treiná-lo. Se os dados de entrada forem tendenciosos, incompletos ou de má qualidade, isso resultará em um sistema de IA com saídas enviesadas ou imprecisas. Portanto, garantir dados de treinamento de alta qualidade e representatividade é essencial para o desenvolvimento de modelos de IA eficazes.

Por uma questão de precisão, é importante destacar que a eficácia de um sistema de IA não depende apenas da qualidade dos dados de treinamento, mas também da sua arquitetura, dos algoritmos utilizados e dos métodos de treinamento. Mesmo quando se dispõe de dados de alta qualidade, um modelo de IA pode produzir resultados insatisfatórios se não for adequadamente projetado, programado e parametrizado.

O viés em IA é um desafio significativo, tanto que até as maiores empresas de tecnologia enfrentaram, e ainda enfrentam, problemas com ele em suas ferramentas.

Em 2015, o aplicativo de Fotos do Google identificou erroneamente um casal como “gorilas”, o que gerou críticas severas à empresa. O Google reconheceu o erro, e seus executivos fizeram declarações públicas de desculpas.

A Microsoft lançou um ChatBot no Twitter em 2016 para interagir com usuários, jovens e adolescentes, e aprender com eles. No entanto, o ChatBot rapidamente começou a fazer declarações ofensivas e racistas. A Microsoft teve que retirar o robô do ar poucas horas após o lançamento e emitir um pedido de desculpas. 

Em 2018, a Amazon enfrentou problemas internos quando sua ferramenta de recrutamento, treinada com dados coletados pelo departamento de recursos humanos ao longo de 10 anos, demonstrou viés. A empresa desativou a ferramenta para solucionar o problema. Em um comunicado, um porta-voz da Amazon esclareceu que o programa nunca foi usado oficialmente. Mesmo assim, temos mais um caso reportado que destaca os riscos de sistemas de IA apresentarem viéses prejudiciais.

Em fevereiro de 2024, logo após o lançamento da sua ferramenta de geração de imagens, o Google publicou um pedido de desculpas pelo que descreveu como "imprecisões em algumas representações de imagens históricas geradas" por sua ferramenta de IA Gemini. A empresa admitiu que suas tentativas de criar uma "ampla variedade" de resultados não foram bem-sucedidas.

# Viés na prática
--> exemplo usando os "sorvetes" no observable. Apenas alterando uma variável de quantidade com gráfico e tabela de previsão.


Essas consequências destacam a importância de uma abordagem cuidadosa e crítica ao desenvolver e implementar modelos de machine learning, incluindo a necessidade de monitoramento contínuo, avaliação de desempenho e implementação de salvaguardas para mitigar riscos.


Riscos associados ao uso da ia generativa
A adoção da IA generativa traz consigo uma série de riscos que já foram mapeados por especialistas. De acordo com Tate Ryan-Mosley, em um artigo publicado no MIT Technology Review's weekly, três desses riscos se destacam como particularmente preocupantes: o aumento e amplificação da desinformação, a presença de viés nos dados de treinamento e nos resultados gerados, bem como a deterioração da privacidade do usuário. Além destes pontos, é relevante acrescentar que a utilização de ferramentas de IA, como o ChatGPT, podem também levar à exposição de informações sensíveis da organização, além de suscitar preocupações relacionadas a plágio e violações de direitos autorais.
Dois exemplos práticos. O primeiro eu extraí da própria documentação do ChatGPT, especificamente no texto que aborda as melhores práticas de segurança. Dentro desse tópico, destaca-se o risco de o ChatGPT gerar textos que, embora possam parecer plausíveis, não são precisos nem correspondem à realidade. Esse fenômeno é conhecido como "alucinações da IA".
O segundo exemplo também envolve o ChatGPT. Em março deste ano, um usuário compartilhou uma imagem que exibia descrições de várias conversas disponíveis na seção “Histórico” do ChatGPT. Na postagem, o usuário afirmou não ter criado essas conversas. O erro, posteriormente confirmado pela OpenAI, apresentava aos usuários títulos de históricos de conversas mantidas com o ChatGPT por outras pessoas.
Os riscos de plágio e violações de direitos autorais tem como origem o processo de treinamento das ferramentas de IA generativa. Muitas dessas ferramentas são alimentadas por extensos bancos de dados que contêm imagens e textos provenientes de diversas fontes, inclusive da internet. Recentemente, a empresa de mídia visual americana, Getty Images, bem como um grupo de artistas, reforçou essa preocupação ao tomarem medidas legais contra os criadores dos geradores de imagem Stable Diffusion e Midjourney. Essas ações judiciais têm como base a alegação de violação de direitos autorais. 
Esses exemplos destacam as complexidades e desafios relacionados ao uso da IA generativa, evidenciando a importância de que todas as organizações estejam adequadamente preparadas para enfrentar e mitigar os riscos inerentes a essa tecnologia. Essa abordagem proativa e cautelosa é fundamental para garantir o uso responsável e seguro destas ferramentas, protegendo os interesses das empresas e minimizando os impactos negativos potenciais.
