---
sidebar_position: 1
---

# Alucinações
Após o lançamento da versão GPT-4o, um colega propôs um desafio visual à inteligência artificial: analisar a foto de uma estátua recém-inaugurada. A resposta peculiar da IA chamou minha atenção. Registrei essa experiência para compartilhar neste curso de IA, pois revela de forma vívida as nuances da inteligência artificial em ação.

# GRAVAÇÃO PRAÇA ATLETICO
--> video <--

Tempo depois repeti a mesma pergunta e o ChatGPT deu a resposta correta. Isso ilustra duas coisas, primeiro como as ferramentas de IA passam por ajustes constantes baseados na sua interação com o usuário e segundo o risco de, pela forma convincente que os modelos de linguagem geram texto, que precisamos estar sempre atento aos seus resultados.

Mas o caso registrado acima, não gerou nenhuma nenhuma consequencia senão a troca de mensagens no whatsapp entre amigos comentando o caso. Ou quem sabe tenha gerado um número suficiente de botões de "resposta insatisfatória" sobre a resposta que ajudou ao sistema a ajustar a sua resposta.

Mas não foi este o caso da AirCanada e dos advogados que usaram o ChatGPT para ajudar no... 

## Erro do ChatBot da Air Canada
Em um caso notável, a companhia aérea Air Canada foi considerada responsável por seu chatbot dar conselhos errados a um passageiro. Em 2022, o chatbot da Air Canada prometeu um desconto que não estava disponível para o passageiro Jake Moffatt, que foi assegurado de que poderia reservar um voo com tarifa integral para o funeral de sua avó e depois solicitar uma tarifa de luto após o fato. No entanto, ao solicitar o desconto, a companhia aérea afirmou que o chatbot estava errado - o pedido precisava ser feito antes do voo - e não ofereceria o desconto. A Air Canada argumentou que Moffatt deveria ter seguido o link fornecido pelo chatbot, onde teria visto a política correta. O tribunal de resoluções civis da Colúmbia Britânica rejeitou esse argumento, decidindo que a Air Canada deveria pagar a Moffatt $812,02 (£642,64) em danos e taxas do tribunal. A decisão estabelece um princípio de senso comum: se você está delegando parte do seu negócio para a IA, você é responsável pelo que ela faz. Esse caso destaca os riscos para as empresas que dependem muito da IA e dos chatbots para interações com os clientes, bem como a necessidade de os passageiros verificarem as informações fornecidas pelos chatbots das companhias aéreas.


## Advogado
Em um caso que gerou polêmica, o advogado Steven A. Schwartz utilizou o ChatGPT para preparar um documento contra a companhia aérea Avianca. No entanto, o ChatGPT compartilhou casos falsos, que Schwartz apresentou no tribunal. O problema veio à tona quando nem os advogados da Avianca nem o juiz conseguiram encontrar as decisões citadas nos casos inventados. Schwartz, inicialmente, negou ter conhecimento da falsidade das informações fornecidas pela IA, alegando que consultou o ChatGPT apenas para "complementar" o documento.
O juiz considerou a situação "sem precedentes" e ordenou uma audiência para avaliar possíveis sanções contra o advogado. 

O advogado Steven A. Schwartz, responsável por um caso contra a Avianca, usou o ChatGPT para consultar outros casos semelhantes ao dele — e o chatbot respondeu.
O profissional, então, perguntou se as informações eram falsas e pediu as fontes de onde os processos anteriores poderiam ser encontrados. Novamente, o ChatGPT respondeu.
Confiando na IA, ele apresentou as informações no tribunal e foi desmascarado, tendo que confessar que usou o chatbot como fonte de pesquisa.



Entendi! Vou ajustar a introdução para incluir esses detalhes e deixá-la engajadora e tecnicamente correta.

Recentemente, recebi uma recomendação intrigante de um amigo torcedor do Clube Athletico Paranaense: testar a mais nova versão do ChatGPT, conhecida como GPT-4o, em cenários inusitados. Decidi desafiar a inteligência artificial com uma tarefa visual: analisar uma foto de uma estátua recém-inaugurada. Para minha surpresa, embora o GPT-4o tenha acertado na descrição da estátua, ele cometeu um erro curioso ao identificar a figura representada. Esse tipo de "alucinação" é um comportamento conhecido em modelos de IA, especialmente quando enfrentam novas situações sem dados suficientes. No entanto, dias depois, o GPT-4o já era capaz de fornecer a resposta correta. Isso levanta questões interessantes sobre a adaptação e a aprendizagem contínua dos modelos de inteligência artificial.

se comportava frente a situações, digamos novas.
É por isso que, não é incomum, que o ChatGPT gere textos bastante convincentes, mas que não são verdadeiros. Isso já tem até um nome: HALUCINAÇÃO. Isso ocorre quando a ferramenta de IA gera informações incorretas ou inventadas. Isso acontece porque o modelo tenta prever a próxima palavra ou frase com base nos padrões que aprendeu, mas às vezes cria respostas que parecem plausíveis, mas não são verdadeiras.



A "alucinação" em IA refere-se a quando um modelo como o ChatGPT gera respostas que são factualmente incorretas ou sem base nos dados de treinamento. Algumas situações comuns em que isso pode ocorrer incluem:

1. **Informações desatualizadas**: Se a pergunta é sobre eventos recentes ou dados que mudaram após meu treinamento.
2. **Ambiguidade**: Quando a pergunta é vaga ou tem múltiplas interpretações, posso escolher uma resposta que não seja a correta.
3. **Falta de dados específicos**: Se o tópico da pergunta não foi suficientemente coberto durante meu treinamento.
4. **Contexto insuficiente**: Quando a conversa não fornece contexto suficiente para uma resposta precisa.
5. **Questões hipotéticas ou ficcionais**: Em perguntas sobre cenários inventados ou que exigem imaginação, posso fornecer respostas que parecem reais, mas são inventadas.
6. **Erros em padrões de linguagem**: Às vezes, a resposta pode seguir um padrão estatístico que é incorreto no contexto específico.
7. **Mistura de informações**: Quando informações de diferentes fontes se misturam, levando a uma resposta que combina fatos reais de forma incorreta.

Essas alucinações ocorrem porque, apesar de meu treinamento ser extenso, ele não é infalível e depende de padrões probabilísticos que nem sempre garantem precisão absoluta.


A vulnerabilidade estatística das ferramentas desenvolvidas usando machine learning, como o ChatGPT, pode levar a várias consequências adicionais além das alucinações. Aqui estão algumas delas:

1. **Bias e Discriminação**: Se os dados de treinamento contêm vieses, o modelo pode perpetuar ou amplificar esses vieses, levando a respostas preconceituosas ou discriminatórias.

2. **Propagação de Desinformação**: Modelos de IA podem gerar ou reforçar informações falsas, o que pode ser particularmente perigoso em contextos sensíveis como saúde, finanças ou política.

3. **Falta de Transparência**: As decisões e respostas geradas pelos modelos de machine learning podem ser difíceis de interpretar ou explicar, levando a uma falta de transparência que pode ser problemática em aplicações críticas.

4. **Dependência Excessiva**: Usuários podem confiar excessivamente nas respostas da IA, mesmo quando a precisão não pode ser garantida, levando a decisões mal informadas.

5. **Segurança e Privacidade**: Modelos de IA podem ser suscetíveis a ataques adversários que exploram vulnerabilidades estatísticas para manipular os resultados, além de possíveis violações de privacidade se os dados de treinamento não forem adequadamente anonimizados.

6. **Sobrefitting e Generalização**: Modelos podem se ajustar excessivamente aos dados de treinamento e não se generalizar bem a novos dados, levando a desempenho inconsistente em situações do mundo real.

7. **Responsabilidade e Ética**: Quando um modelo de IA gera resultados incorretos ou prejudiciais, pode ser difícil atribuir responsabilidade, levantando questões éticas sobre o uso dessas tecnologias.

Essas consequências destacam a importância de uma abordagem cuidadosa e crítica ao desenvolver e implementar modelos de machine learning, incluindo a necessidade de monitoramento contínuo, avaliação de desempenho e implementação de salvaguardas para mitigar riscos.